{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "TF_5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4EFYBUgPwyx"
      },
      "source": [
        "# TensorFlow crash course\n",
        "### **PART 7**"
      ],
      "id": "V4EFYBUgPwyx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP5lPZj5gYY1"
      },
      "source": [
        "## Custom training loops"
      ],
      "id": "UP5lPZj5gYY1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngqBR0hsqkV1"
      },
      "source": [
        "### Load MNIST"
      ],
      "id": "ngqBR0hsqkV1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBzzJxu8qnNd",
        "outputId": "b2290235-78ca-4444-e9c8-8b4ff597037e"
      },
      "source": [
        "import numpy as np \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist \n",
        "from tensorflow.keras.utils import to_categorical \n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
        "X_train, X_test = X_train[:10000], X_test[:1000]\n",
        "y_train, y_test = y_train[:10000], y_test[:1000]\n",
        "X_train, X_test = X_train.reshape(-1, 28*28) / 255.0, X_test.reshape(-1, 28*28) / 255.0\n",
        "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "id": "LBzzJxu8qnNd",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10000, 784) (1000, 784) (10000, 10) (1000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_YAztfKgb7W",
        "outputId": "304bd4b2-c198-4246-b052-37ee0b078afb"
      },
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "\n",
        "#  Implement a simple classification model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(784, ), activation=\"relu\"))\n",
        "model.add(Dense(64, activation=\"relu\"))\n",
        "model.add(Dense(32, activation=\"relu\"))\n",
        "model.add(Dense(10, activation=\"softmax\"))                  \n",
        "model.summary()"
      ],
      "id": "A_YAztfKgb7W",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                330       \n",
            "=================================================================\n",
            "Total params: 111,146\n",
            "Trainable params: 111,146\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQmM-24yp8Nr"
      },
      "source": [
        "def random_batch(X, y, batch_size=32): #  Select batches randomly\n",
        "    idx = np.random.randint(len(X), size=batch_size)\n",
        "    return X[idx], y[idx]"
      ],
      "id": "pQmM-24yp8Nr",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9s4QG-WdqUb-"
      },
      "source": [
        "n_epochs = 10\n",
        "batch_size = 32\n",
        "n_steps = len(X_train) // batch_size\n",
        "optimizer = Nadam(learning_rate=0.01)\n",
        "loss_fn = CategoricalCrossentropy()"
      ],
      "id": "9s4QG-WdqUb-",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFwaXZwXqZnz",
        "outputId": "8f792b0d-5708-4ad8-9483-e260ba6fe3c9"
      },
      "source": [
        "for epoch in range(1, n_epochs + 1): #  For epochs\n",
        "    print(f\"Epoch {epoch}/{n_epochs}\") \n",
        "    losses = []\n",
        "    for step in range(1, n_steps + 1): #  For batches\n",
        "        X_batch, y_batch = random_batch(X_train, y_train)\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pred = model(X_batch, training=True) #  Make prediction\n",
        "            loss = loss_fn(y_batch, y_pred) #  Compute the loss\n",
        "            losses.append(loss)\n",
        "            '''Note : model.trainable_variables and model.trainable_weights are almost\n",
        "            the same except in some special layers like BatchNomalization'''\n",
        "        gradients = tape.gradient(loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "    print(f\"loss : {tf.reduce_mean(losses)}\")\n",
        "    losses = []"
      ],
      "id": "zFwaXZwXqZnz",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "loss : 0.4633212387561798\n",
            "Epoch 2/10\n",
            "loss : 0.21464140713214874\n",
            "Epoch 3/10\n",
            "loss : 0.1655184030532837\n",
            "Epoch 4/10\n",
            "loss : 0.1328078806400299\n",
            "Epoch 5/10\n",
            "loss : 0.1285111904144287\n",
            "Epoch 6/10\n",
            "loss : 0.11869336664676666\n",
            "Epoch 7/10\n",
            "loss : 0.1372356116771698\n",
            "Epoch 8/10\n",
            "loss : 0.09185465425252914\n",
            "Epoch 9/10\n",
            "loss : 0.07806721329689026\n",
            "Epoch 10/10\n",
            "loss : 0.09695207327604294\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
