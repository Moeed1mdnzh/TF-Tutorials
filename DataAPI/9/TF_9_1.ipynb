{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "V4EFYBUgPwyx",
   "metadata": {
    "id": "V4EFYBUgPwyx"
   },
   "source": [
    "# TensorFlow crash course\n",
    "### **PART 9.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FNYZIZMEw-LJ",
   "metadata": {
    "id": "FNYZIZMEw-LJ"
   },
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tVap8ZkeyP_E",
   "metadata": {
    "id": "tVap8ZkeyP_E"
   },
   "source": [
    "For this part we'll use the California housing dataset and then deeply go through the main idea\n",
    "before we get into the main concept, we first need to accomplish the following steps\n",
    "1. Shuffle the data\n",
    "2. Split the data into training, validation and test sets\n",
    "3. Split the sets into many .csv files <br />\n",
    "It's alot of work but fortunately the file splitter will do the job !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "zLQUayjyxPD6",
   "metadata": {
    "id": "zLQUayjyxPD6"
   },
   "outputs": [],
   "source": [
    "import splitter as spt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "y-BaEf3_2DqC",
   "metadata": {
    "id": "y-BaEf3_2DqC"
   },
   "outputs": [],
   "source": [
    "data, num_of_feats, column_names = spt.load(\"housing.csv\")  # Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dBYHdB732vkZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dBYHdB732vkZ",
    "outputId": "1f2a3b6c-0454-457a-fa82-c9c619a9f4f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of features : 20640\n"
     ]
    }
   ],
   "source": [
    "data = spt.shuffle(data, num_of_feats)  # Shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "_6HgaGXz215r",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_6HgaGXz215r",
    "outputId": "4c33a1c6-ebeb-46ab-d37a-ffc849cafcb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size : 15480\n",
      "Val size : 1032\n",
      "Test size : 4128\n"
     ]
    }
   ],
   "source": [
    "train, val, test = spt.split_to_sets(data, num_of_feats)  # Create training, validation, test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lYlPAj_b3oBf",
   "metadata": {
    "id": "lYlPAj_b3oBf"
   },
   "outputs": [],
   "source": [
    "spt.decompose(train, val, test, column_names)  # Create the subfolders for batch csv files"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TF_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
