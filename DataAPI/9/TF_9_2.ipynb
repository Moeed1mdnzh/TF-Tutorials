{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "V4EFYBUgPwyx",
   "metadata": {
    "id": "V4EFYBUgPwyx"
   },
   "source": [
    "# TensorFlow crash course\n",
    "### **PART 9.2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rkbQ2wBo6I4S",
   "metadata": {
    "id": "rkbQ2wBo6I4S"
   },
   "source": [
    "## Interleaving lines from multiple files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ujA6K4nz6Yxy",
   "metadata": {
    "id": "ujA6K4nz6Yxy"
   },
   "source": [
    "### Get file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hSmo2dpI6cj4",
   "metadata": {
    "id": "hSmo2dpI6cj4"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "train_filepath = \"dataset/train/my_train_*.csv\"\n",
    "filepath_dataset = tf.data.Dataset.list_files(train_filepath, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RUiK5IFj7hEb",
   "metadata": {
    "id": "RUiK5IFj7hEb"
   },
   "source": [
    "### Interleave\n",
    "#### In this case we'll interleave from 5 files at the same time skipping the header using the skip() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "Ei4iNDAA7lxo",
   "metadata": {
    "id": "Ei4iNDAA7lxo"
   },
   "outputs": [],
   "source": [
    "# Go through 5 files -> Read them skipping the header -> Interleave\n",
    "n_readers = 5\n",
    "dataset = filepath_dataset.interleave(\n",
    "    lambda filepath : tf.data.TextLineDataset(filepath).skip(1),  \n",
    "    cycle_length=n_readers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "SeYAs7DHcpPm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SeYAs7DHcpPm",
    "outputId": "dd3a9e0e-d98d-437a-84f3-9a47667fdf74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'-121.72,37.98,5.0,7105.0,1143.0,3523.0,1088.0,5.0468,168800.0'\n",
      "b'-121.55,38.55,10.0,6227.0,1164.0,2909.0,1077.0,4.106,115900.0'\n",
      "b'-118.47,34.0,42.0,1271.0,301.0,574.0,312.0,3.1304,340500.0'\n",
      "b'-120.43,34.87,21.0,2131.0,329.0,1094.0,353.0,4.6648,193000.0'\n",
      "b'-118.29,34.18,52.0,1602.0,265.0,667.0,251.0,5.0489999999999995,323500.0'\n"
     ]
    }
   ],
   "source": [
    "# Lets check the result out\n",
    "for line in dataset.take(5):\n",
    "    print(line.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fM6DVvujc7sD",
   "metadata": {
    "id": "fM6DVvujc7sD"
   },
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "FiLPnuSGdMya",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FiLPnuSGdMya",
    "outputId": "50c122ee-8b71-460e-81d6-147e90742581"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(8,), dtype=float32, numpy=\n",
       " array([1.0353299 , 0.99768853, 0.15421276, 0.36362094, 0.8554898 ,\n",
       "        0.1831137 , 0.60330486, 0.2901311 ], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1,), dtype=float32, numpy=array([172500.], dtype=float32)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "train_distr = np.load(\"dataset/train_distr.npy\")  # Load the pre-computed mean and std\n",
    "X_mean, X_std = np.split(train_distr, 2)\n",
    "n_inputs = 8\n",
    "\n",
    "def preprocess(line):\n",
    "    defs = [0.] * n_inputs + [tf.constant([], dtype=tf.float32)]\n",
    "    fields = tf.io.decode_csv(line, record_defaults=defs)  # Parse each line of the csv data\n",
    "    x = tf.stack(fields[:-1])\n",
    "    y = tf.stack(fields[-1:])\n",
    "    return (x - X_mean[:8]) / X_std[:8], y \n",
    "\n",
    "# Let's test the created function\n",
    "preprocess(b\"-121.78,37.68,17.0,3112.0,872.0,1392.0,680.0,3.0222,172500.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oP-NyfVsj170",
   "metadata": {
    "id": "oP-NyfVsj170"
   },
   "source": [
    "### Helper function\n",
    "This function is supposed to help us by preprocessing the data using the following techniques\n",
    "1. Data loading\n",
    "2. Data splitting\n",
    "3. Data preprocessing\n",
    "4. Data shuffling\n",
    "5. Data repeating\n",
    "6. Data batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "LZAarl9jk0gg",
   "metadata": {
    "id": "LZAarl9jk0gg"
   },
   "outputs": [],
   "source": [
    "def csv_reader_dataset(filepaths, repeat=1, n_readers=5,\n",
    "                        n_read_threads=None, shuffle_buffer_size=10000,\n",
    "                        n_parse_threads=5, batch_size=32):\n",
    "    dataset = tf.data.Dataset.list_files(filepaths)\n",
    "    dataset = dataset.interleave(\n",
    "    lambda filepath: tf.data.TextLineDataset(filepath).skip(1),\n",
    "    cycle_length=n_readers, num_parallel_calls=n_read_threads)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "    \"\"\"Note : With prefetching, the CPU and the GPU work in parallel: as the GPU works\n",
    "    on one batch, the CPU works on the next\"\"\"\n",
    "    return dataset.batch(batch_size).prefetch(1)  # This tells TensorFlow to be always one batch ahead"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TF_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
